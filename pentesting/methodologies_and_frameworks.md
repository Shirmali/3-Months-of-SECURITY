# Pentest Methodology Comparison — OSSTMM · OWASP · NIST

---

## Quick summary 

* **NIST SP 800-115**: governance and engagement process (Planning → Discovery → Vulnerability Analysis → Exploitation → Reporting).
* **OWASP (WSTG / ASVS / Top 10)**: detailed web/API/mobile test cases and developer-friendly remediation guidance.
* **OSSTMM**: measurement-focused, multi-channel methodology producing auditable metrics (RAV / STAR) for operational security.

Combine them: use NIST for process & ROE, OWASP for concrete web/app tests, OSSTMM when you need quantitative measurement or multi-channel coverage.

---

## Comparison table

| Aspect             |                                                                                                  OSSTMM | OWASP (WSTG / ASVS / Top 10)                                           | NIST SP 800-115                                                                          |
| ------------------ | ------------------------------------------------------------------------------------------------------: | ---------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| Publisher          |                                                                                                  ISECOM | OWASP Foundation                                                       | NIST (US Gov)                                                                            |
| Focus / Scope      | Operational security across channels (Physical, Wireless, Telecoms, Data Networks, Applications, Human) | Web, APIs, Mobile; developer-focused test cases & standards            | Broad technical testing: networks, systems, applications, scanning & exploitation phases |
| Methodology type   |                                                                      Measurement & metrics (RAV / STAR) | Test-case / requirements based (WSTG, ASVS)                            | Process-oriented engagement lifecycle                                                    |
| Phases / Structure |                                                          Channel-based modules; results feed RAV / STAR | WSTG test cases grouped by area; ASVS levels define verification depth | 5 phases: Planning → Discovery → Vulnerability Analysis → Exploitation → Reporting       |
| Outputs            |                                                         STAR report, quantitative exposure/trust scores | Findings mapped to Top 10 / ASVS, WSTG test references                 | Formal test report + evidence; recommended remediation                                   |
| Best use cases     |                                     Mixed-scope audits, physical/wireless + IT, audit-grade measurement | App/API/mobile pentests; dev-focused remediation                       | Formal engagements, contracts, enterprise/government testing                             |
| Strengths          |                                                          Repeatable metrics; multi-channel; audit-ready | Practical test cases; widely adopted by devs                           | Strong planning & ROE, clear phases for safe testing                                     |
| Weaknesses         |                                                                             Heavyweight; learning curve | App-only focus; no built-in scoring                                    | Lighter on concrete test cases — supplement with WSTG                                    |

---

## When to pick which

* **Pick NIST SP 800-115** if you need: formal engagement templates, rules-of-engagement, scheduling & legal controls, or you’re working with conservative enterprise/government customers.
* **Pick OWASP** if your scope is primarily web, API or mobile and you want detailed test cases and developer-actionable remediation.
* **Pick OSSTMM** if you need measurement and numerical evidence across multiple channels (physical, wireless, telephony, human), or you must produce auditable scores that can be compared across time.

Most mature programs use a *combination* of all three.

---

## Recommended combined workflow (copy into a pentest plan)

**Phase 0 — Pre-engagement / Contracting**

* Stakeholders: client PO, technical contact, legal contact.
* Deliverables: signed Rules of Engagement (ROE), Authorization letter, scope (IP ranges, hostnames, API routes), out-of-scope list, test windows, emergency contacts.
* Methodology references included: NIST SP 800-115 (engagement), ASVS level (1–3) or Top-10 priorities, OSSTMM channels scoped.

**Phase 1 — Reconnaissance & Discovery (NIST + WSTG)**

* Passive discovery: WHOIS, subdomain enumeration, open-source intelligence (OSINT).
* Active discovery: `nmap -sS -p- --open -T4 <target>`, directory fuzzing (`gobuster` / `ffuf`), map API endpoints.
* Deliverable: discovery inventory (hosts, services, endpoints, assets), mapping to ASVS controls to be tested.

**Phase 2 — Vulnerability Analysis (OWASP WSTG)**

* Run scanners (credentialed/uncredentialed) and triage findings.
* Manually verify using WSTG test-cases for each candidate finding (injection, session management, ACLs, etc.).
* Deliverable: verified findings list with WSTG/ASVS references.

**Phase 3 — Exploitation / Validation (NIST + WSTG constraints)**

* Execute safe PoCs for high/critical findings following ROE. Capture evidence (screenshots, logs, timestamps, commands).
* If OSSTMM is in-scope: log every access/attempt per channel and capture data necessary to compute RAV metrics.
* Deliverable: PoC evidence pack, activity logs.

**Phase 4 — Metrics & STAR (Optional OSSTMM)**

* If using OSSTMM, compute RAV / exposure scores and produce the STAR (security test audit report) with quantified changes and per-channel scoring.
* Deliverable: STAR PDF + numeric metrics + mapping to findings.

**Phase 5 — Reporting & Debrief (NIST-style deliverables + OWASP mapping)**

* Produce: Executive summary (Top 10 mapping), Technical findings (WSTG/ASVS references), Remediation roadmap (developer-friendly), Evidence appendix (commands, screenshots, logs), and OSSTMM metrics if applicable.
* Conduct a debrief call with stakeholders and provide timelines for re-test.

---

## Report template / Table of Contents (README-friendly)

```
1. Executive Summary
   - One-paragraph risk posture
   - Top 3 findings
   - OSSTMM RAV / STAR (if produced)

2. Scope & Rules of Engagement
   - Dates, contacts, in-scope assets, excluded items

3. Methodology Used
   - NIST SP 800-115 (engagement phases)
   - OWASP WSTG / ASVS (test-cases / verification)
   - OSSTMM (metrics & STAR) — if applicable

4. Findings (sorted by risk)
   - Title (OWASP Top 10 category)
   - Affected assets / endpoints
   - Description & impact
   - Repro steps / PoC (safe)
   - Risk rating & ASVS requirement ID
   - Remediation steps (code/config examples)

5. Evidence Appendix
   - Commands used, scans, timestamps, screenshots

6. Metrics & Measurements
   - OSSTMM RAV calculation (if used)
   - Changes vs baseline (if prior test exists)

7. Next Steps & Closing
   - Re-test plan, remediation priorities, contact info
```

---

## Example quick-check commands (adapt to ROE)

* TCP port discovery:

```bash
nmap -sS -p- -T4 --open -oA nmap_discovery <target-ip>
```

* Service + SSL checks:

```bash
nmap -sV --script ssl-enum-ciphers -p 443 <target>
```

* Web directory discovery:

```bash
gobuster dir -u https://<target>/ -w /path/wordlist.txt -x php,html,txt -a 'Mozilla/5.0' -t 50
```

* Basic web fuzzing:

```bash
ffuf -u https://<target>/FUZZ -w /path/wordlist.txt
```

---



